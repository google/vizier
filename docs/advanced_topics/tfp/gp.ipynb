{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOp-9XGzKYIx"
      },
      "source": [
        "# Bayesian Optimization Modeling\n",
        "The goal of this tutorial is to introduce Bayesian optimization workflows in OSS Vizier, including the underlying TensorFlow Probability (TFP) components and JAX/Flax functionality. The target audience is researchers and practitioners already well-versed in Bayesian optimization, who want to **define and train their own Gaussian Process surrogate models** for Bayesian optimization in OSS Vizier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzyCnk228YLU"
      },
      "source": [
        "## Additional resources for TFP\n",
        "If you're new to TFP, a good place to start is [A tour of TensorFlow Probability](https://www.tensorflow.org/probability/examples/A_Tour_of_TensorFlow_Probability). TFP began as a TensorFlow-only library, but now has a [JAX backend](https://www.tensorflow.org/probability/examples/TensorFlow_Probability_on_JAX) that is entirely independent of TensorFlow (such that \"Tensor-Friendly Probability might be a better backronym). This Colab uses TFP's JAX backend (see the \"Imports\" cell for how to import it)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdxDxRtW8YtL"
      },
      "source": [
        "## Additional resources for Flax\n",
        "OSS Vizier's Bayesian Optimization models are defined as [Flax](https://flax.readthedocs.io/en/latest/) modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrW3fCmYv9MC"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import jax\n",
        "from jax import numpy as jnp, random, tree_util\n",
        "import numpy as np\n",
        "import optax\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "\n",
        "from vizier._src.jax import optimizers\n",
        "from vizier._src.jax import stochastic_process_model as spm\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "tfpk = tfp.math.psd_kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw_N-81ANozw"
      },
      "source": [
        "## Defining a GP surrogate model and hyperparameters\n",
        "To write a GP surrogate model, first write a coroutine that yields parameter specifications (`ModelParameter`) and returns a GP distribution. Downstream, the parameter specifications are used to define Flax module parameters. The inputs to the coroutine function represent the index points of the GP (in the remainder of this Colab, we refer to \"inputs\" and \"index points\" interchangeably).\n",
        "\n",
        "The rationale for the coroutine design is that it lets us automate the application of the parameter constraint and regularization functions (corresponding to hyperpriors, e.g.), and enables simultaneous specification of the model parameters and how their values are used to instantiate a GP.\n",
        "\n",
        "The following cell shows a coroutine defining a GP with a squared exponential kernel and two parameters: the length scale of the kernel and the observation noise variance of the GP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMBRd_msy5Iq"
      },
      "outputs": [],
      "source": [
        "def simple_gp_coroutine(inputs=None):\n",
        "  length_scale = yield spm.ModelParameter.from_prior(\n",
        "      tfd.Gamma(1., 1., name='length_scale'))\n",
        "  amplitude = 2.  # Non-trainable parameters may be defined as constants.\n",
        "  kernel = tfpk.ExponentiatedQuadratic(\n",
        "      amplitude=amplitude, length_scale=length_scale)\n",
        "  observation_noise_variance = yield spm.ModelParameter(\n",
        "      init_fn=random.normal,\n",
        "      constraining_fn=tfb.Softplus(),\n",
        "      regularizer=lambda x: x**2,\n",
        "      name='observation_noise_variance')\n",
        "  return tfd.GaussianProcess(\n",
        "      kernel,\n",
        "      index_points=inputs,\n",
        "      observation_noise_variance=observation_noise_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXWmOlqUy9Jc"
      },
      "source": [
        "`ModelParameter` may be used to define hyperpriors; here, the length scale parameter has a Gamma prior. This is equivalent to defining a `ModelParameter` with a regularizer that computes the Gamma negative log likelihood and an initialization function that samples from the Gamma distribution.\n",
        "\n",
        "Observation noise variance, which is passed to the Gaussian process and represents the scalar variance of zero-mean Gaussian noise in the observed labels, is not given a `tfd.Distribution` prior. Instead, it has its initialization, constraining, and regularization functions defined individually.\n",
        "\n",
        "The constraining function allows for parameter optimization in an unconstrained space. For the length scale prior, this function is the \"default event space bijector\" of the TFP distribution (each TFP distribution has a constraining bijector that maps the real line to the support of the distribution). Note that the initialization function is in the unconstrained space (for parameters with a prior, a sample from the prior is passed to the inverse of the constraining bijector)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0l40fQxzOMS"
      },
      "source": [
        "### Exercise: Write a GP model\n",
        "Write an ARD Gaussian Process model with three parameters: `signal_variance`, `length_scale`, and `observation_noise_variance`. (This is a slightly simplified version of the Vizier GP.)\n",
        "- `signal_variance` and `observation noise_variance` are both:\n",
        "  - regularized by the function $f(x) = 0.01\\log(x)^2$\n",
        "  - initialized with samples from a standard Normal distribution.\n",
        "  - constrained with softplus.\n",
        "- `signal_variance` parameterizes a Matern 5/2 kernel, where the amplitude of the kernel is the square root of `signal_variance`. Use [`tfpk.MaternFiveHalves`](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/math/psd_kernels/matern.py#L414).\n",
        "- `length_scale` has a $LogNormal(0, 1)$ prior for each dimension. Assume there are 4 dimensions, and use [`tfd.Sample`](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/distributions/sample.py) to build a 4-dimensional distribution consisting of IID LogNormal distributions. (Note that the `length_scale` parameter is a vector -- all other parameters are scalars.)\n",
        "- In TFP, ARD kernels are implemented with [`tfpk.FeatureScaled`](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/math/psd_kernels/feature_scaled.py), with `scale_diag` representing the length scale along each dimension.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XluuKkwFD4ns"
      },
      "outputs": [],
      "source": [
        "def vizier_gp_coroutine(inputs=None):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj7pJgA8zU4y"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "def vizier_gp_coroutine(inputs=None):\n",
        "  \"\"\"A coroutine that follows the `ModelCoroutine` protocol.\"\"\"\n",
        "  signal_variance = yield spm.ModelParameter(\n",
        "      init_fn=random.normal,\n",
        "      constraining_fn=tfb.Softplus(),\n",
        "      regularizer=lambda x: 0.01 * jnp.log(x)**2,\n",
        "      name='signal_variance')\n",
        "  length_scale = yield spm.ModelParameter.from_prior(\n",
        "      tfd.Sample(\n",
        "          tfd.LogNormal(loc=0, scale=1.),\n",
        "          sample_shape=[4],\n",
        "          name='length_scale'))\n",
        "  kernel = tfpk.MaternFiveHalves(\n",
        "      amplitude=jnp.sqrt(signal_variance),\n",
        "      length_scale=length_scale,\n",
        "      validate_args=True)\n",
        "  observation_noise_variance = yield spm.ModelParameter(\n",
        "      init_fn=random.normal,\n",
        "      constraining_fn=tfb.Softplus(),\n",
        "      regularizer=lambda x: 0.01 * jnp.log(x)**2,\n",
        "      name='observation_noise_variance')\n",
        "  return tfd.GaussianProcess(\n",
        "      kernel=kernel,\n",
        "      index_points=inputs,\n",
        "      observation_noise_variance=observation_noise_variance,\n",
        "      validate_args=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTBcby0FzZjT"
      },
      "source": [
        "To build a GP Flax module, instantiate a `StochasticProcessModel` with a GP coroutine as shown below. The module runs the coroutine in the `setup` and `__call__` methods to initialize the parameters and then instantiate the GP object with the given parameters.\n",
        "\n",
        "Recall that Flax modules have two primary methods: `init`, which initializes parameters, and `apply`, which computes the model's forward pass given a set of parameters and input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW8iIFI3j6ad"
      },
      "outputs": [],
      "source": [
        "model = spm.StochasticProcessModel(coroutine=vizier_gp_coroutine)\n",
        "\n",
        "# Sample some fake data.\n",
        "# TODO: Use Branin or similar instead?\n",
        "# Assume we have `num_points` observations, each with `dim` features.\n",
        "num_points = 12\n",
        "dim = 4\n",
        "\n",
        "# Sample a set of index points.\n",
        "index_points = np.random.normal(size=[num_points, dim]).astype(np.float32)\n",
        "\n",
        "# Sample function values observed at the index points\n",
        "observations = np.random.normal(size=[num_points]).astype(np.float32)\n",
        "\n",
        "# Call the Flax module's `init` method to obtain initial parameter values.\n",
        "init_params = model.init(random.PRNGKey(0), index_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssgl1ULJlQf6"
      },
      "source": [
        "Recall that the Flax model parameters are in an unconstrained space. Parameters like `observation_noise_variance`, that are positive, may have negative unconstrained values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR1mQBLZltro"
      },
      "outputs": [],
      "source": [
        "print(init_params['params'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61BuqfvblrfI"
      },
      "source": [
        "To compute the constrained values of the parameters, use `build_constraining_bijector`. This function returns a `JointMap` multipart bijector (see the section on Bijectors below), whose `forward` method takes a dict of unconstrained parameters and returns a dict of constrained parameters (applying the constraining functions defined above). The constrained values of the variance parameters are positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Uold94l_yJ"
      },
      "outputs": [],
      "source": [
        "parameter_constraining_bijector = spm.ConstrainModelParameters(\n",
        "    vizier_gp_coroutine)\n",
        "constrained_init_params = parameter_constraining_bijector(init_params['params'])\n",
        "print(constrained_init_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPks4ZJVzj0O"
      },
      "source": [
        "To instantiate a GP with a set of parameters and index points, use the Flax module's `apply` method. `apply` also returns the regularization losses for the parameters, in `mutables`. The regularization losses are treated as mutable state because they are recomputed internally with each forward pass of the model. For more on mutable state in Flax, see [this](https://flax.readthedocs.io/en/latest/guides/state_params.html) tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeFrRqXemxMa"
      },
      "outputs": [],
      "source": [
        "gp, mutables = model.apply(\n",
        "    init_params,\n",
        "    index_points,\n",
        "    mutable=['losses', 'predictive'])\n",
        "assert isinstance(gp, tfd.GaussianProcess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBRCCmBzvkRg"
      },
      "source": [
        "## Optimizing hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMxnVHOyIxHI"
      },
      "source": [
        "### Exercise: Loss function\n",
        "Write down a loss function that takes a parameters dict and returns the loss value, using `model.apply`. The function will close over the observed data.\n",
        "\n",
        "The loss should be the sum of the GP negative log likelihood and the regularization losses. The regularization loss values are computed when the module is called, using the `ModelParameter` regularization functions. They are stored in a mutable variable collection called `\"losses\"`, using the Flax method [`sow`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html?highlight=sow#flax.linen.Module.sow)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYOpS460J4SC"
      },
      "outputs": [],
      "source": [
        "def loss_fn(params):\n",
        "  ...\n",
        "  return loss, {}  # Return an empty dict as auxiliary state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kx1AbzwvoA3"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "def loss_fn(params):\n",
        "  gp, mutables = model.apply({'params': params},\n",
        "                              index_points,\n",
        "                              mutable=['losses', 'predictive'])\n",
        "  loss = (-gp.log_prob(observations) +\n",
        "          jax.tree_util.tree_reduce(jnp.add, mutables['losses']))  # add the regularization losses.\n",
        "  return loss, {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiVS3CZoAWoF"
      },
      "source": [
        "The gradients of the loss have the same structure as the `params` dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X31_PEZ_3XM"
      },
      "outputs": [],
      "source": [
        "grads = jax.grad(loss_fn, has_aux=True)(init_params['params'])[0]\n",
        "print(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z1WCXl0DUZg"
      },
      "source": [
        "We can use `jax.tree_util` to take a step along the gradient (though in practice, with Optax, we can use `update` and `apply_updates` to update the parameters at each train step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz66WUMmDkI3"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "updated_params = jax.tree_util.tree_map(\n",
        "    lambda p, g: p - learning_rate * g,\n",
        "    init_params['params'],\n",
        "    grads)\n",
        "print(updated_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN3hdzETKARf"
      },
      "source": [
        "Optimize hyperparameters with a Vizier optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w7_N0fPJ9p7"
      },
      "outputs": [],
      "source": [
        "setup = lambda rng: model.init(rng, index_points)['params']\n",
        "optimize_model = optimizers.OptaxTrainWithRandomRestarts(\n",
        "    optax.adam(5e-3),\n",
        "    epochs=100,\n",
        "    verbose=False,\n",
        "    random_restarts=50)  # Choose the best of 50 initializations.\n",
        "optimal_params = optimize_model(setup, loss_fn, rng=random.PRNGKey(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohHdQfjtCqIq"
      },
      "source": [
        "## Predict on new inputs, conditional on observations\n",
        "To compute the posterior predictive GP on unseen points, conditioned on observed data, use the `precompute_predictive` and `predict` methods of the Flax module. `precompute_predictive` must be called first; it runs and stores the Cholesky decomposition of the kernel matrix for the observed data. `predict` then returns a posterior predictive GP at new index points, avoiding recomputation of the Cholesky."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNIKrHskCsRf"
      },
      "outputs": [],
      "source": [
        "# Precompute the Cholesky.\n",
        "_, pp_state = model.apply(\n",
        "    {'params': optimal_params},\n",
        "    index_points,\n",
        "    observations,\n",
        "    mutable=['predictive'],\n",
        "    method=model.precompute_predictive)\n",
        "\n",
        "# Predict on new index points.\n",
        "predictive_index_points = np.random.normal(size=[5, dim]).astype(np.float32)\n",
        "pp_dist, _ = model.apply(\n",
        "    {'params': optimal_params, **pp_state},\n",
        "    predictive_index_points,\n",
        "    mutable=['predictive'],\n",
        "    method=model.predict)\n",
        "\n",
        "# `predict` returns a TFP distribution, whose mean, variance, and samples we can\n",
        "# use to compute an acquisition function.\n",
        "assert pp_dist.mean().shape == (5,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPM-5NKX5cjM"
      },
      "source": [
        "## Deeper dive on selected topics in TFP\n",
        "As shown above, the Flax GP model makes use of a number of TFP components:\n",
        "- Distributions specify parameter priors (e.g. `tfd.Gamma`). The stochastic process model itself is also a TFP distribution, `tfd.GaussianProcess`.\n",
        "- Bijectors (e.g. `tfb.Softplus`) are used to constrain parameters for optimization, and may also be used for input/output warping.\n",
        "- PSD kernels (e.g. `tfpk.ExponentiatedQuadratic`) specify the kernel function for the stochastic process.\n",
        "\n",
        "The next sections of this Colab introduce these and how they're used in Bayesopt modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuU739_PDX6i"
      },
      "source": [
        "### `tfd.GaussianProcess` and friends\n",
        "The stochastic process Flax modules return a TFP distribution in the [Gaussian Process](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/distributions/gaussian_process.py#L93) family (an instance of `tfd.GaussianProcess`, `tfd.StudentTProcess`, or `tfde.MultiTaskGaussianProcess`).\n",
        "\n",
        "This Colab doesn't go into detail on TFP distributions, since advanced usage and implementation of distributions is rarely required for Bayesopt modeling with Vizier. For an overview of TFP distributions, see [TensorFlow Distributions: A Gentle Introduction](https://www.tensorflow.org/probability/examples/TensorFlow_Distributions_Tutorial).\n",
        "\n",
        "Some of the methods of the Gaussian Process distribution are demonstrated below. [Gaussian Process Regression in TFP](https://www.tensorflow.org/probability/examples/Gaussian_Process_Regression_In_TFP) is also worth reading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1ULLbnpDj_2"
      },
      "outputs": [],
      "source": [
        "# Build a kernel function (see \"PSD kernels\" section below) and GP.\n",
        "kernel = tfpk.MaternFiveHalves(\n",
        "    amplitude=2.,\n",
        "    length_scale=0.3,\n",
        "    validate_args=True  # Run additional runtime checks; possibly expensive.\n",
        "    )\n",
        "observation_noise_variance = jnp.ones([], dtype=observations.dtype)\n",
        "gp = tfd.GaussianProcess(\n",
        "    kernel,\n",
        "    index_points=index_points,\n",
        "    observation_noise_variance=observation_noise_variance,\n",
        "    always_yield_multivariate_normal=True,  # See commentary below.\n",
        "    cholesky_fn=lambda x: tfp.experimental.distributions.marginal_fns.retrying_cholesky(x)[0],  # See commentary below.\n",
        "    validate_args=True)\n",
        "\n",
        "# Take 4 samples from the GP at the index points.\n",
        "s = gp.sample(4, seed=random.PRNGKey(0))\n",
        "assert s.shape == (4, 12)\n",
        "\n",
        "# Compute the log likelihood of the sampled values.\n",
        "lp = gp.log_prob(s)\n",
        "assert lp.shape == (4,)\n",
        "\n",
        "# GPs can also be instantiated without index points, in which case the index\n",
        "# points must be passed to method calls.\n",
        "gp_no_index_pts = tfd.GaussianProcess(\n",
        "    kernel,\n",
        "    observation_noise_variance=observation_noise_variance)\n",
        "s = gp_no_index_pts.sample(index_points=index_points, seed=random.PRNGKey(0))\n",
        "\n",
        "# Predictive GPs conditioned on observations can be built with\n",
        "# `GaussianProcess.posterior_predictive`. The Flax module's\n",
        "# `precompute_predictive` and `predict` methods call this GP method.\n",
        "gprm = gp.posterior_predictive(\n",
        "    observations=observations,\n",
        "    predictive_index_points=predictive_index_points)\n",
        "\n",
        "# `gprm` is an instance of `tfd.GaussianProcessRegressionModel`. This class can\n",
        "# also be instantiated directly (as a side note -- this isn't necessary for\n",
        "# modeling with Vizier).\n",
        "same_gprm = tfd.GaussianProcessRegressionModel(\n",
        "    kernel,\n",
        "    index_points=predictive_index_points,\n",
        "    observation_index_points=index_points,\n",
        "    observations=observations,\n",
        "    observation_noise_variance=observation_noise_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUwFGcRxPa2F"
      },
      "source": [
        "Aside from the kernel, index points, and noise variance, there are two constructor args of `GaussianProcess` to be aware of:\n",
        "- `always_yield_multivariate_normal`. By default, if there is only a single index point (`index_points` has shape `[1, d]`), then the Gaussian process has a univariate marginal distribution, so methods like `mean`, `stddev`, and `sample` will return a scalar. (If `index_points` has shape `[n, d]` the output of these methods will have shape `[n]`). To override the behavior for the univariate case and return arrays of shape `[1]` instead of scalars, set `always_yield_multivariate_normal=True`.\n",
        "- `cholesky_fn` is a callable that takes a matrix and returns a Cholesky-like lower triangular factor. The default function adds a jitter of 1e-6 to the diagonal and then calls `jnp.linalg.cholesky`. An alternative, used in the Vizier GP, is `tfp.experimental.distributions.marginal_fns.retrying_cholesky`, which adds progressively larger jitter until the Cholesky decomposition succeeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QkOf892WVX3"
      },
      "source": [
        "### A side note on batch shape in TFP\n",
        "tl;dr: Don't worry about batch shape.\n",
        "\n",
        "TFP objects have a notion of batch shape, which is useful for vectorized computations. For more on this, see [Understanding TensorFlow Distributions Shapes](https://www.tensorflow.org/probability/examples/Understanding_TensorFlow_Distributions_Shapes).\n",
        "\n",
        "For the purposes of Bayesopt in Vizier, JAX's `vmap` means that our TFP objects can have a single parameterization with empty batch shape. For example, in the following loss function takes a scalar `amplitude`, and the kernel and GP both have empty batch shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIhhwTvYWmVG"
      },
      "outputs": [],
      "source": [
        "def loss_fn(amplitude):  # `a` is a scalar.\n",
        "  k = tfpk.ExponentiatedQuadratic(amplitude=amplitude)  # batch shape []\n",
        "  gp = tfd.GaussianProcess(k, index_points=index_points)   # batch shape []\n",
        "  return -gp.log_prob(observations)\n",
        "\n",
        "initial_amplitude = np.random.uniform(size=[50])\n",
        "\n",
        "losses = jax.vmap(loss_fn)(initial_amplitude)\n",
        "assert losses.shape == (50,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQyX3m2I4QSv"
      },
      "source": [
        "We could also vectorize the loss computation by using a batched GP. In this simple case, the code is identical except that `vmap` is removed. Now, the kernel and GP represent a \"batch\" of kernels and GPs, each with different parameter values. Working with batch shape requires additional accounting on the part of the user to ensure that parameter shapes broadcast correctly, the correct dimensions are reduced over, etc. For Vizier's use case, we find it simpler to rely on `vmap`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFWBcAw84daN"
      },
      "outputs": [],
      "source": [
        "def loss_fn(amplitude):  # `a` has shape [50].\n",
        "  k = tfpk.ExponentiatedQuadratic(amplitude=amplitude)  # batch shape [50]\n",
        "  gp = tfd.GaussianProcess(k, index_points=index_points)   # batch shape [50]\n",
        "  return -gp.log_prob(observations)\n",
        "\n",
        "initial_amplitude = np.random.uniform(size=[50])\n",
        "\n",
        "# No vmap.\n",
        "losses = loss_fn(initial_amplitude)\n",
        "assert losses.shape == (50,)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1bhe9vVJps8t8IsIU4sbInYvcQPgBlLWn",
          "timestamp": 1667943943389
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
